{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af0d3549-4ec7-4371-a43d-7f4a8d576fba",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# ERT Data Assimilation\n",
    "**Notebook to reproduce outputs from Mary et al 2023**\n",
    "\n",
    "Authors: Benjamin Mary, ORCID: 0000-0001-7199-2885\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fbcf3b-9f6c-4c52-9cda-9523f7db4666",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "This is a notebook to reproduce outputs from  [Mary et al (2023)](link to come) \n",
    "\n",
    ":::{important}\n",
    "# Cite this work\n",
    "We believe in a community-driven approach of open-source tools that are\n",
    "composable and extensible. If you use this notebook cite the work as:\n",
    "> \n",
    ":::\n",
    "\n",
    "While in the full article different scenarios are considered, for simplicity here we only show the application for the scenario 1 and 2 describe below.\n",
    "\n",
    "The notebooks describes: \n",
    "\n",
    "1. **Preprocessing step**: build a mesh mimicking a rhizotron experiment.\n",
    "\n",
    "2. **Prepare for Data Assimilation**\n",
    "   - 2.1 Read observations\n",
    "   - 2.1.2 Create matrice covariances\n",
    "   - 2.2 Perturbate\n",
    "   - 2.3 Define mapping operator\n",
    "   \n",
    "3. **Simulation**: solve the surface-subsurface flow.\n",
    "\n",
    "\n",
    "4. **Plot outputs**: analysis of the results\n",
    "   - Saturation with uncertainties\n",
    "   - Assimilation quality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab4405-d151-47b0-b7a3-ba6d744beb22",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    ":::{admonition} What you should already know\n",
    "In order to complete this tutorial, you should be relatively familiar with using the pyCATHY for:\n",
    "- Building a mesh from a DEM (See {doc}`../content/SSHydro/index` for more information.)\n",
    "- Basics of Data Assimilation\n",
    "- ...\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ccbec7-bbeb-401c-bcf7-310f401336d9",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "```{warning}\n",
    "Found a bug üêõ/ a typo ? [Email me](mailto:benjamin.mary@unipd.it)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2582b2-686c-45c6-81c8-e1c9ad38cb5b",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "We start by importing the required packages: \n",
    "- `matplotlib` in order to plot the data;\n",
    "- `cathy_tools` is the main object controlling the simulation;\n",
    "- `pyCATHY.DA.cathy_DA` allows you to create a Data Assimilation Object (with all CATHY tools modules inheritence);\n",
    "- `perturbate` module to add perturbation to the model parameters;\n",
    "- `perturbate_parm` function to perturbate a given parameter.\n",
    "- `pyCATHY.DA.observations` module to read, prepare observations for DA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5f95d16-7e94-4365-965e-f68720826be2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from pyCATHY.DA.cathy_DA import DA\n",
    "from pyCATHY.DA import perturbate\n",
    "from pyCATHY.DA.cathy_DA import perturbate_parm, dictObs_2pd\n",
    "from pyCATHY.DA.observations import read_observations, prepare_observations, make_data_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c81cfb-6fdc-4328-9c06-0fd8eb91b895",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üèÅ <span style=\"font-weight: bold\">Initiate CATHY object</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "üèÅ \u001b[1mInitiate CATHY object\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#%% Init CATHY DA object \n",
    "# -----------------------\n",
    "simu_DA = DA(dirName='.', \n",
    "            prj_name='Maryetal_2023_WRR',\n",
    "            notebook=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12fb2e0-6a75-4583-a9b3-79f588777cc7",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## 1. Preprocessing\n",
    "\n",
    "Build an adequate mesh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86140f94-9ec8-4c7f-8f8c-34e84fe3d3d3",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## 2. Prepare Data Assimilation\n",
    "\n",
    "First we define a dictionnary `scenario` describing which and how of the model parameter are **perturbated** and **updated**. Two cases are considered: \n",
    "- `uniroot_ic`\n",
    "- `uniroot_ic_PCREF_ZROOT_updPCREF_ZROOT`\n",
    "\n",
    "\n",
    ":::{admonition} Feddes parameters perturbation\n",
    "In order to perturbate Feddes parameters we use typical range of possible values for **PCREF** as describe in ??. As for **ZROOT** the perturbation must be bounded within the limit of the simulation region (rhizotron domain). This condition is applied thanks to the dictionnary key `per_bounds`.\n",
    ":::\n",
    "\n",
    "\n",
    "- `per_type`\n",
    "- `per_name`\n",
    "- `per_nom`\n",
    "- `per_sigma`\n",
    "- `sampling_type`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2aae3870-d1ae-4755-b532-638adcedb8a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uniroot_ic': {'per_type': [None], 'per_name': ['ic'], 'per_nom': [-5], 'per_mean': [-5], 'per_sigma': [1.75], 'transf_type': [None], 'listUpdateParm': ['St. var.']}, 'uniroot_ic_PCREF_ZROOT_updPCREF_ZROOT': {'per_type': [None, None, 'additive'], 'per_name': ['ic', 'ZROOT', 'PCREF'], 'per_nom': [-5, 0.4, -4], 'per_mean': [-5, 0.4, -1], 'per_sigma': [1.75, 0.005, 1.55], 'per_bounds': [None, {'min': 0, 'max': 0.5}, None], 'sampling_type': ['normal', 'normal', 'normal'], 'transf_type': [None, None, None], 'ref_scenario': 'model_uni_root_ArchiePert0_f1_noise5', 'listUpdateParm': ['St. var.', 'ZROOT', 'PCREF']}}\n"
     ]
    }
   ],
   "source": [
    "scenario = {\n",
    "    \n",
    "            # scenario without parameter update\n",
    "            # -------------------------------------\n",
    "            'uniroot_ic': {'per_type': [None],\n",
    "                                'per_name':['ic'],\n",
    "                                'per_nom':[-5],\n",
    "                                'per_mean':[-5],\n",
    "                                'per_sigma': [1.75],\n",
    "                                'transf_type':[None],\n",
    "                                'listUpdateParm': ['St. var.'],\n",
    "                                },\n",
    "    \n",
    "            # scenario with parameter update\n",
    "            # -------------------------------------\n",
    "            'uniroot_ic_PCREF_ZROOT_updPCREF_ZROOT': \n",
    "                                                        {'per_type': [None,None,'additive'], # this is a list of the same size than ['ic', 'ZROOT','PCREF']\n",
    "                                                         'per_name':['ic', 'ZROOT','PCREF'],\n",
    "                                                         'per_nom':[-5,0.4,-4],\n",
    "                                                         'per_mean':[-5,0.4,-1],    \n",
    "                                                         'per_sigma': [1.75,5e-3,1.55],\n",
    "                                                         'per_bounds': [None,{'min':0,'max':0.5},None],\n",
    "                                                         'sampling_type': ['normal']*3,\n",
    "                                                         'transf_type':[None,None,None],\n",
    "                                                         'ref_scenario': 'model_uni_root_ArchiePert0_f1_noise5', \n",
    "                                                         'listUpdateParm': ['St. var.', 'ZROOT','PCREF']\n",
    "                                                         },    \n",
    "            }\n",
    "print(scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e917228-3cd5-462f-a243-4e32650a7807",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### 2.1 Import ERT observations\n",
    "\n",
    "Read the csv files produced by pygimli\n",
    "\n",
    "```{tip}\n",
    "    need to call `read_observations` as many times as variable to perturbate \n",
    "    return a dict merging all variable perturbate to parse into prepare_DA\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d74bbf7d-21b8-4452-8595-a1be92948f86",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meshERT' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ERT observations metadata\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[1;32m      3\u001b[0m ERT    \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$ERT$\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# units\u001b[39;00m\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOhm$\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# units transfer_resistances\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward_mesh_vtk_file\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mmeshERT\u001b[49m, \u001b[38;5;66;03m# path to the ERT mesh (vtk file compatible with pygimli or resipy)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequenceERT\u001b[39m\u001b[38;5;124m'\u001b[39m: sequenceERT, \u001b[38;5;66;03m# path to the ERT sequence  (file compatible with pygimli or resipy)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstrument\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSyscal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m# Instrument\u001b[39;00m\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_format\u001b[39m\u001b[38;5;124m'\u001b[39m: data_format, \u001b[38;5;66;03m# format (raw or preprocessed)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         }\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Initiate an empty dictionnary\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m     14\u001b[0m data_measure \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'meshERT' is not defined"
     ]
    }
   ],
   "source": [
    "# ERT observations metadata\n",
    "# -------------------------\n",
    "ERT    = {\n",
    "            'data_type': '$ERT$', # units\n",
    "            'units': '$\\Ohm$', # units transfer_resistances\n",
    "            'forward_mesh_vtk_file': meshERT, # path to the ERT mesh (vtk file compatible with pygimli or resipy)\n",
    "            'sequenceERT': sequenceERT, # path to the ERT sequence  (file compatible with pygimli or resipy)\n",
    "            'instrument': 'Syscal', # Instrument\n",
    "            'data_format': data_format, # format (raw or preprocessed)\n",
    "        }\n",
    "\n",
    "# Initiate an empty dictionnary\n",
    "# -----------------------------\n",
    "data_measure = {}\n",
    "for i, tt in enumerate(time_sampled):\n",
    "\n",
    "    # csv file observation generated by pygimli\n",
    "    filename = os.path.join(pathERT, prjERT, 'ER_predicted_sol_t' + str(i) + '.csv')\n",
    "    data_measure = read_observations(\n",
    "                                    data_measure,\n",
    "                                    filename, \n",
    "                                    data_type = 'ERT', \n",
    "                                    data_err = args.dataErr, # instrumental error\n",
    "                                    show=True,\n",
    "                                    tA=tt,\n",
    "                                    obs_cov_type='data_err', #data_err\n",
    "                                    elecs=elecs,\n",
    "                                    meta=ERT\n",
    "                                    ) # data_err  reciprocal_err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68ae60e-96f0-4b30-b53f-598001f1fbb5",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "#### 2.1.1 Create observation covariance matrice \n",
    "\n",
    "```{tip}\n",
    "    For the ERT case we used a diagonal matrice\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535aec55-0e0d-4b7d-bea2-fa26dceb0840",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_measure_df = dictObs_2pd(data_measure) \n",
    "data_cov_stacked_times = [] # need to define covariance matrice for each assimilation times\n",
    "for i, tt in enumerate(time_sampled):\n",
    "    data_cov = data_measure_df['data_cov'].iloc[i]\n",
    "    data_cov_stacked_times.append(data_cov)\n",
    "\n",
    "simu_DA.stacked_data_cov = data_cov_stacked_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027a4b6d-7b43-4436-991e-d25ea8947f7c",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### 2.2 Perturbate \n",
    "\n",
    "```{tip}\n",
    "    need to call `read_observations` as many times as variable to perturbate \n",
    "    return a dict merging all variable perturbate to parse into prepare_DA\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35c8a2e4-e0ad-489d-8104-83ddab80a430",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function perturbate in module pyCATHY.DA.perturbate:\n",
      "\n",
      "perturbate(simu_DA, scenario, NENS)\n",
      "    Write a list of dictionaries, each containing all the informations on how to\n",
      "    perturbate the parameters based on the scenario to consider\n",
      "\n",
      "[{'type_parm': 'ic', 'nominal': -5, 'mean': -5, 'sd': 1.75, 'units': 'pressure head $(m)$', 'sampling_type': 'normal', 'ensemble_size': 3, 'per_type': None, 'savefig': 'ic.png'}]\n"
     ]
    }
   ],
   "source": [
    "help(perturbate.perturbate)\n",
    "NENS = 3\n",
    "list_pert = perturbate.perturbate(simu_DA, scenario['uniroot_ic'], NENS)\n",
    "print(list_pert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efeaeb5-adb7-4e90-b190-bc25c5021285",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "var_per_dict_stacked = {}\n",
    "for dp in list_pert:\n",
    "\n",
    "    np.random.seed(1)\n",
    "\n",
    "    # need to call perturbate_var as many times as variable to perturbate\n",
    "    # return a dict merging all variable perturbate to parse into prepare_DA\n",
    "    var_per_dict_stacked = perturbate_parm(\n",
    "                                var_per_dict_stacked,\n",
    "                                parm=dp, \n",
    "                                type_parm = dp['type_parm'], # can also be VAN GENUCHTEN PARAMETERS\n",
    "                                mean =  dp['mean'],\n",
    "                                sd =  dp['sd'],\n",
    "                                sampling_type =  dp['sampling_type'],\n",
    "                                ensemble_size =  dp['ensemble_size'], # size of the ensemble\n",
    "                                per_type= dp['per_type'],\n",
    "                                savefig= os.path.join(prj_name,\n",
    "                                                      prj_name + dp['savefig'])\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb956b30-933e-4bbb-9814-5185b385fbd8",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### 2.3 Define mapping operator \n",
    "\n",
    "Use `set_Archie_parm` with the `simu_DA` object\n",
    "\n",
    "\n",
    "\n",
    "```{tip} Need another mapper?\n",
    "    It is possible to quickly add a mapper to the DA. \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76c313e-08fc-4c3e-88f2-2cbb39ce5ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "simu_DA.set_Archie_parm(\n",
    "                            rFluid_Archie=solution_parm['rFluid_Archie'],\n",
    "                            a_Archie=solution_parm['a_Archie'],\n",
    "                            m_Archie=solution_parm['m_Archie'],\n",
    "                            n_Archie=solution_parm['n_Archie'],\n",
    "                            pert_sigma_Archie=solution_parm['pert_sigma_Archie']\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4757e4f-9e9a-4002-9058-caffd51c4119",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### 3. Run sequential DA\n",
    "\n",
    "Simply use `run_DA_sequential()` with the `simu_DA` object\n",
    "\n",
    "\n",
    "\n",
    "Required arguments are:\n",
    "- **dict_obs**: dictionnary of observations\n",
    "- **list_assimilated_obs**: list of observation to assimilate \n",
    "- **list_parm2update**: list of parameters to update \n",
    "\n",
    "Possible **optionnal** arguments are: \n",
    "- **parallel**: if True use multiple cores to run many realisations at the same time\n",
    "- **DA_type**: type of data assimilation\n",
    "- **threshold_rejected**: threshold above which the simulation stops (i.e. ensemble of rejected realisation too big)\n",
    "- **damping**: add damping to inflate the covariance matrice\n",
    "\n",
    "\n",
    "```{tip}\n",
    "    During the execution **useful informations are displayed on the console** in order to follow the state of the DA. You can for example appreciated how many ensemble are rejected.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58de4ac-14ac-4a84-9c5f-aec769f5ca6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "simu_DA.run_DA_sequential(\n",
    "                              DTMIN=1e2,\n",
    "                              DELTAT=1e2,\n",
    "                              parallel=parallel,    \n",
    "                              dict_obs= data_measure,\n",
    "                              list_assimilated_obs='all', # default\n",
    "                              list_parm2update=scenarii[list(scenarii)[Snb]]['listUpdateParm'],\n",
    "                              DA_type=DA_type, #'pf_analysis', # default\n",
    "                              dict_parm_pert=var_per_dict_stacked,\n",
    "                              open_loop_run=open_loop_run,\n",
    "                              threshold_rejected=80,\n",
    "                              damping=args.damping                    \n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410385de-5583-407d-a462-67aef390a377",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### 4. Plot outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab3e2c5-7cf2-4ef5-a6b2-b208517a89f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = simu.load_pickle_backup()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
